{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#clone the repo so you can mess around with it:\n","!git clone https://github.com/apapiu/guided-diffusion-keras.git\n","! pip install git+https://github.com/openai/CLIP.git\n","\n","#this is where the data is:\n","!git lfs install\n","!git clone https://huggingface.co/apapiu/diffusion_model_aesthetic_keras\n","! pip3 install tensorflow==2.13.0\n","import os\n","import sys\n","import numpy as np\n","import pandas as pd\n","\n","from tensorflow.keras.datasets import mnist, fashion_mnist, cifar10, cifar100\n","from keras.utils.vis_utils import plot_model\n","\n","from tensorflow import keras\n","from matplotlib import pyplot as plt\n","\n","####local imports:\n","sys.path.append(\"/content/guided-diffusion-keras/guided_diffusion\")\n","from denoiser import get_network\n","from utils import batch_generator, plot_images, get_data, preprocess\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#########\n","# CONFIG:\n","#########\n","\n","train_model=True\n","\n","image_size = 64\n","num_channels = 3\n","epochs = 50\n","class_guidance = 4\n","\n","# architecture\n","channels = 96\n","channel_multiplier = [1, 2, 3, 4]\n","block_depth = 2\n","emb_size = 512  # CLIP/label embedding\n","num_classes = 12  # only used if precomputed_embedding=False\n","attention_levels = [0, 0, 1, 0]\n","\n","embedding_dims = 32\n","embedding_max_frequency = 1000.0\n","\n","precomputed_embedding = True\n","save_in_drive = False\n","widths = [c * channels for c in channel_multiplier]\n","\n","###train process config:\n","batch_size = 64\n","num_imgs = 36 #num imgs to test on - should be a square - 25, 64, 100 etc.\n","row = int(np.sqrt(num_imgs))\n","\n","validation_num = 100 #don't train on first validation_num images so we can use as visual validation\n","train_size = 40000 #only train on a subset of train data - do this for faster results..\n","\n","learning_rate = 0.0003\n","\n","MODEL_NAME = \"model_test_aesthetic\"\n","from_scratch = False #if False will load model from model path and continue training\n","file_name = \"from_huggingface\"\n","\n","data_dir = '/content/diffusion_model_aesthetic_keras'\n","captions_path = os.path.join(data_dir, \"captions.csv\")\n","imgs_path = os.path.join(data_dir, \"imgs.npy\")\n","embedding_path = os.path.join(data_dir, \"embeddings.npy\")\n","\n","if save_in_drive:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    drive_path = '/content/drive/MyDrive/'\n","    home_dir = os.path.join(drive_path, MODEL_NAME)\n","else:\n","    home_dir = MODEL_NAME\n","\n","if not os.path.exists(home_dir):\n","    os.mkdir(home_dir)\n","\n","model_path = os.path.join(home_dir, MODEL_NAME + \".h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","print(model_path,  tf. __version__)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["##################################\n","###########Loading Data And Model:\n","##################################\n","\n","#load the data from a numpy file file:\n","captions = pd.read_csv(captions_path)\n","train_data, train_label_embeddings = np.load(imgs_path), np.load(embedding_path)\n","#train_data, train_label_embeddings = get_data(npz_file_name=file_name, prop=0.6, captions=False)\n","print(train_data.shape)\n","\n","labels = train_label_embeddings[:num_imgs]\n","\n","np.random.seed(100)\n","rand_image = np.random.normal(0, 1, (num_imgs, image_size, image_size, num_channels))\n","\n","if from_scratch:\n","    autoencoder = get_network(image_size,\n","                              widths,\n","                              block_depth,\n","                              num_classes=num_classes,\n","                              attention_levels=attention_levels,\n","                              emb_size=emb_size,\n","                              num_channels=num_channels,\n","                              precomputed_embedding=precomputed_embedding)\n","\n","    autoencoder.compile(optimizer=\"adam\", loss=\"mae\")\n","else:\n","    autoencoder = keras.models.load_model(model_path)\n","\n","##################\n","#Some data checks:\n","##################\n","\n","print(\"Number of pamaters is {0}\".format(autoencoder.count_params()))\n","pd.Series(train_data[:1000].ravel()).hist(bins=80)\n","plt.show()\n","print(\"Original Validation Images:\")\n","plot_images(preprocess(train_data[:num_imgs]), nrows=int(np.sqrt(num_imgs)))\n","plot_model(autoencoder, to_file=os.path.join(home_dir, 'model_plot.png'),\n","           show_shapes=True, show_layer_names=True)\n","print(\"Validation Captions:\")\n","print(captions[:num_imgs][\"0\"].values)\n","print(\"Generating Images below:\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","import keras\n","from keras import layers, models\n","from keras.layers import Input\n","\n","\n","def vstack(tensors_list):\n","    return tf.concat(tensors_list, axis=0)\n","\n","class Dafa2_t:\n","\n","    def __init__(self, denoiser, class_guidance, batch_size=64): \n","        rand_image_input = keras.Input((64,64,3,))\n","        label_input = keras.Input((1, 512,))\n","\n","        zero_array = tf.zeros((1, 512,))\n","\n","        noise_in2 = tf.convert_to_tensor(np.array([0.95358411])[:, None, None, None])\n","\n","        print(rand_image_input.shape, noise_in2.shape, label_input.shape)\n","\n","        x1 = vstack([rand_image_input, rand_image_input])\n","        y1 = vstack([noise_in2,noise_in2])\n","        z1 = vstack([label_input, label_input])\n","\n","        print(x1.shape, y1.shape, z1.shape)\n","        inputs = [x1,y1,z1]\n","\n","        out = denoiser.call(inputs)\n","\n","        self.out = out[1]\n","\n","        import tf2onnx\n","\n","        spec0 = tf.TensorSpec([64,64,3], tf.float32, name='input_0')\n","        spec1 = tf.TensorSpec([1, 512], tf.float32, name='input_1')\n","\n","\n","        input_signature = [spec0, spec1]\n","\n","        onnx_model, _ = tf2onnx.convert.from_keras(self.out, input_signature=input_signature, inputs_as_nchw=[\n","            'input_0',\n","            'input_1'\n","        ],\n","                                                   opset=12\n","                                                   )\n","\n","\n","        # output = OriginalModelAsLayer(denoiser)(inputs, batch_size)\n","\n","        self.denoiser = denoiser\n","        self.class_guidance = class_guidance\n","        self.diffusion_steps = diffusion_steps\n","        #TODO: parametrize this better:\n","        self.noise_levels = 1 - np.power(np.arange(0.0001, 0.99, 1 / self.diffusion_steps), 1 / 3)\n","\n","        self.noise_levels[-1] = 0.01\n","        self.batch_size = batch_size\n","        print(\"noise_levels\", self.noise_levels)\n","\n","\n","    def reverse_diffusion(self, seeds, label):\n","        \"\"\"Reverse Guided Diffusion on a matrix of random images (seeds). Returns generated images\"\"\"\n","        print(\"testtttt\",label.shape)\n","\n","        new_img = seeds\n","\n","        curr_noise, next_noise = self.noise_levels[0], self.noise_levels[1]\n","\n","        # predict original denoised image:\n","        # x0_pred = self.predict_x_zero(new_img, label, curr_noise)\n","        x_t = new_img\n","        noise_level = curr_noise\n","\n","        # we use 0 for the unconditional embedding:\n","        num_imgs = len(x_t)\n","        label_empty_ohe = np.zeros(shape=label.shape)\n","\n","        # predict x0:\n","        noise_in = np.array([noise_level])[:, None, None, None]\n","\n","        # print(\"noise_in2\", noise_in)\n","\n","        # TODO: can we do some of this in tensorflow?\n","        # concatenate the conditional and unconditional inputs to speed inference:\n","\n","        nn_inputs = [np.vstack([x_t, x_t]),\n","                     np.vstack([noise_in, noise_in]),\n","                     np.vstack([label, label_empty_ohe])]\n","\n","        x0_pred = self.denoiser.predict(nn_inputs, batch_size=self.batch_size)\n","\n","        # x0_pred_label = x0_pred[:num_imgs]\n","        x0_pred = x0_pred[1:]\n","\n","        print(x0_pred)\n","\n","        return x0_pred"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","sys.path.append(\"/content/guided-diffusion-keras/guided_diffusion\")\n","from utils import batch_generator, plot_images, get_data, preprocess, slerp, get_text_encodings\n","\n","\n","\n","prompt = \"mountain landscape with cloudy sky with a house\"\n","\n","class_guidance = 5\n","\n","diffusion_steps = 2\n","\n","seed =  1\n","\n","big_diffuser = Dafa2_t(autoencoder,\n","                        class_guidance=class_guidance,\n","                       )\n","print(\"hadi hadi\")\n","\n","print(big_diffuser.noise_levels)\n","\n","num_imgs = 1\n","np.random.seed(seed) #- 100/120 baseline\n","rand_image = np.random.normal(0, 1, (num_imgs, image_size, image_size, num_channels))\n","\n","prompt=[prompt]*num_imgs\n","\n","labels = get_text_encodings(prompt, model)\n","\n","print(len(labels[0]))\n","\n","imgs = big_diffuser.reverse_diffusion(rand_image, labels)\n","plot_images(imgs, nrows=int(np.sqrt(num_imgs)), save_name=prompt[0], size=12)"]}],"metadata":{"language_info":{"name":"python"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
